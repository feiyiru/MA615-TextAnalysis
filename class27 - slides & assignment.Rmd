---
title: "Text Analysis"
author: "MA615"
date: "Nov 2, 2018"
output:
  slidy_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Today  

 * Quick review of chapters 1-4  
 * Form groups  
 * Pick a book and produce an analysis  
 * Produce a document or slide presentation for your analysis  



## Chapter 2  


![](tidy-text-mining-master/images/tidyflow-ch2a.png)  


\vspace{.1inch}  
* Sentiment analysis  
* Graphical depiction of the emotional flow of a novel  
* Word clouds  

## Chapters 3 & 4

### Ch 3

* Word frequency and rank
* tf-idf words
* Zipf's Law


### Ch 4  

* n-grams  
* n-gram wrangling  
* bi-gram frequency  
* Network visualizations of bi-gram  


## Class Assignment

Pick a book and carry out the analysis that was done in the first four chapters of 
[Text Mining with R](https://www.tidytextmining.com/tidytext.html).

Make sure that you get a book or a collection downloaded as text.  Make sure that you can organize the tokens as shown in Chapter 1 and that you can at least generate a histogram using the work frequencies.  Go as far as you can with the analysis.  Produce notes about what you have done and how.  Include comments about what your analysis shows.

## Chapter 1  

![](tidy-text-mining-master/images/tidyflow1a.png)  

* Tidy data applied to text  
* Word frequency  
* Correlation as a measure of similarity  

# Read the book: <The Unprotected Species>
```{r}
library(gutenbergr)
library(dplyr)
text <- gutenberg_download(32036)
text_df <- tibble(text = text)
```

Break the text into individual tokens(tokenization) and transform it to a tidy data structure.
```{r}
library(tidytext)
library(stringr)

text %>% unnest_tokens(word, text)


#Keep track of lines in original format and a chapter to find where these chapters are:
book <- text %>% mutate(linenumber=row_number(),
                                           chpater=cumsum(str_detect(text, regex("^(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$", ignore_case = TRUE))))
book

#restructure it in the one-token-per-row format:
tidy_book <- book %>% unnest_tokens(word, text)
tidy_book
```

Remove stop words that not useful for an analysis
```{r}
library(stopwords)
tidy_book <- tidy_book %>% anti_join(stop_words)
```  

Find the most common words in the book
```{r}
tidy_book %>% count(word, sort = TRUE)
```

Create a visualization of the most common words
```{r}
library(ggplot2)
tidy_book %>% count(word,sort = TRUE) %>%
  filter(n > 20) %>%
  mutate(word=reorder(word,n)) %>%
  ggplot(aes(word,n)) +
  geom_col() +
  xlab("Frequency of words occurance") +
  coord_flip()
```

## Chapter 2


Assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.
```{r}
library(tidytext)

get_sentiments("afinn")
```
Categorizes words in a binary fashion into positive and negative categories.
```{r}
get_sentiments("bing")
```

Categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.
```{r}
get_sentiments("nrc")
```

What are the most common fear words in Chapter 6?
```{r}
library(janeaustenr)
library(dplyr)
library(stringr)



nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_book %>%
  filter(chpater == "6") %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)
```
Plot these sentiment scores across the plot trajectory of each chapter
```{r}
library(tidyr)
jane_austen_sentiment <- tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(chpater, index = linenumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = chpater)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~chpater, ncol = 2, scales = "free_x")
```
```{r}
chapter.6 <- tidy_book %>% 
  filter(chpater == "6")

chapter.6
```
How the sentiment changes across the narrative arc of Chapter 6.
```{r}
afinn <- chapter.6 %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 10) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(chapter.6 %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing"),
                          chapter.6 %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
Both lexicons have more negative than positive words, but the ratio of negative to positive words is higher in the Bing lexicon than the NRC lexicon.   


Find out how much each word contributed to each sentiment.
```{r}
bing_word_count <- tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_count
```
```{r}
bing_word_count %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

Look at the most common words in book as a whole again
```{r}
library(wordcloud)

tidy_book %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 50))
```
Do the sentiment analysis to tag positive and negative words, then find the most common positive and negative words
```{r}
library(reshape2)

tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 5)
```

##Chapter 3
What are the most commonly used words in selected book.
```{r}
library(dplyr)
library(tidytext)

book_words <- tidy_book %>%

  count(chpater, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(chpater) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words




```

This is exactly what term frequency is.
```{r}
library(ggplot2)

ggplot(book_words, aes(n/total, fill = chpater)) +
  geom_histogram(show.legend = FALSE, bins = 100) +
  xlim(NA, 0.0009) + 
  facet_wrap(~chpater, ncol = 2, scales = "free_y")

```

Examine Zipf's law for selected book.
```{r}
freq_by_rank <- book_words %>% 
  group_by(chpater) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank

```
```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = chpater)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

tf-idf
```{r}
book_words <- book_words %>%
  bind_tf_idf(word, chpater, n)

book_words
```
Look at terms with high tf-idf in selected book.
```{r}
book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```
Look at a visualization for these high tf-idf words
```{r}
book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(chpater) %>% 
  top_n(6) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = chpater)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~chpater, ncol = 2, scales = "free") +
  coord_flip()
```

# Chapter 4
By seeing how often word X is followed by word Y, we can then build a model of the relationships between them; examining pairs of two consecutive words, often called “bigrams”
```{r}
library(tidytext)
library(janeaustenr)
bigrams <- text %>% unnest_tokens(bigram, text, token = "ngrams", n=2)
bigrams

#count frequency of 2 consecutive words
bigrams %>% count(bigram, sort = TRUE)
```

separate bigrams into two columns
```{r}
library(tidyr)

bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```
For 3 consecutive sequences of 3 words
```{r}
text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)
```
Interested in the most common "hawkins" mentioned in the book:
```{r}
bigrams_filtered %>%
  filter(word2 == "hawkins") %>%
  count(gutenberg_id, word1, sort = TRUE)

bigram_tf_idf <- bigrams_united %>%
  count(gutenberg_id, bigram) %>%
  bind_tf_idf(bigram, gutenberg_id, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf

bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

```{r}
#since cannot download package AFINN, used bing instead.
Bing <- get_sentiments("bing")
Bing

not_words <- bigrams_separated %>%
  filter(word1 == "of") %>%
  inner_join(Bing, by = c(word2 = "word")) %>%
  count(word2, sentiment, sort = TRUE)

not_words
```

Visulization
Since in package bing, there is no value as in AFINN, thus, we cannot measure numeric sentiment value right here.
```{r}
library(ggplot2)

not_words %>%
  mutate(contribution = n) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n, fill = n > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment value * number of occurrences") +
  coord_flip()

negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(Bing, by = c(word2 = "word")) %>%
  count(word1, word2, sentiment, sort = TRUE)
```

```{r}
library(igraph)

# original counts
bigram_counts

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 5) %>%
  graph_from_data_frame()

bigram_graph
```

Convert an igraph object into a ggraph. Showing those occurred more than 5 times and where neither word was a stop word.
```{r}
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(igraph)
library(ggraph)

count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}
library(gutenbergr)
book1 <- gutenberg_download(32036)
library(stringr)
bigrams1 <- book1 %>%
  count_bigrams()

# filter out rare combinations, as well as digits
bigrams1 %>%
  filter(n > 5,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()
```
```{r}
words <- text %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

words

library(widyr)

# count words co-occuring within sections
word_pairs <- words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs %>%
  filter(item1 == "gallifa")


# we need to filter for at least relatively common words first
word_cors <- words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors

word_cors %>%
  filter(item1 == "gallifa")
```

```{r}
word_cors %>%
  filter(item1 %in% c("gallifa", "hawkins", "gnomes", "samuels")) %>%
  group_by(item1) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()


set.seed(2016)

word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

### book sites

http://freecomputerbooks.com/

https://en.wikisource.org/wiki/Main_Page

http://www.gutenberg.org/


## Text analysis links   


[CRAN Task View: Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)


[RcmdrPlugin.temis: Graphical Integrated Text Mining Solution](https://cran.r-project.org/web/packages/RcmdrPlugin.temis/index.html)

[Text mining with R github](https://github.com/dgrtwo/tidy-text-mining)


[r-break-corpus-into-sentences](https://stackoverflow.com/questions/18712878/r-break-corpus-into-sentences)

[tokenizers](https://cran.r-project.org/web/packages/tokenizers/README.html)


[Introduction to the tokenizers Package](https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html)



[Package ‘tokenizers’](https://cran.r-project.org/web/packages/tokenizers/tokenizers.pdf)











