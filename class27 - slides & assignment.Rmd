---
title: "Text Analysis"
author: "MA615"
date: "Nov 2, 2018"
output:
  slidy_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Today  

 * Quick review of chapters 1-4  
 * Form groups  
 * Pick a book and produce an analysis  
 * Produce a document or slide presentation for your analysis  



## Chapter 2  


![](tidy-text-mining-master/images/tidyflow-ch2a.png)  


\vspace{.1inch}  
* Sentiment analysis  
* Graphical depiction of the emotional flow of a novel  
* Word clouds  

## Chapters 3 & 4

### Ch 3

* Word frequency and rank
* tf-idf words
* Zipf's Law


### Ch 4  

* n-grams  
* n-gram wrangling  
* bi-gram frequency  
* Network visualizations of bi-gram  


## Class Assignment

Pick a book and carry out the analysis that was done in the first four chapters of 
[Text Mining with R](https://www.tidytextmining.com/tidytext.html).

Make sure that you get a book or a collection downloaded as text.  Make sure that you can organize the tokens as shown in Chapter 1 and that you can at least generate a histogram using the work frequencies.  Go as far as you can with the analysis.  Produce notes about what you have done and how.  Include comments about what your analysis shows.

## Chapter 1  

![](tidy-text-mining-master/images/tidyflow1a.png)  

* Tidy data applied to text  
* Word frequency  
* Correlation as a measure of similarity  

# Read the book: <The Unprotected Species>
```{r}
library(gutenbergr)
library(dplyr)
text <- gutenberg_download(32036)
text_df <- tibble(text = text)
```

Break the text into individual tokens(tokenization) and transform it to a tidy data structure.
```{r}
library(tidytext)
library(stringr)

text %>% unnest_tokens(word, text)


#Keep track of lines in original format and a chapter to find where these chapters are:
book <- text %>% mutate(linenumber=row_number(),
                                           chpater=cumsum(str_detect(text, regex("^(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$", ignore_case = TRUE))))
book

#restructure it in the one-token-per-row format:
tidy_book <- book %>% unnest_tokens(word, text)
tidy_book
```

Remove stop words that not useful for an analysis
```{r}
library(stopwords)
tidy_book <- tidy_book %>% anti_join(stop_words)
```  

Find the most common words in the book
```{r}
tidy_book %>% count(word, sort = TRUE)
```

Create a visualization of the most common words
```{r}
library(ggplot2)
tidy_book %>% count(word,sort = TRUE) %>%
  filter(n > 20) %>%
  mutate(word=reorder(word,n)) %>%
  ggplot(aes(word,n)) +
  geom_col() +
  xlab("Frequency of words occurance") +
  coord_flip()
```

## Chapter 2


Assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.
```{r}
library(tidytext)

get_sentiments("afinn")
```
Categorizes words in a binary fashion into positive and negative categories.
```{r}
get_sentiments("bing")
```

Categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.
```{r}
get_sentiments("nrc")
```

What are the most common fear words in Chapter 6?
```{r}
library(janeaustenr)
library(dplyr)
library(stringr)



nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_book %>%
  filter(chpater == "6") %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)
```
Plot these sentiment scores across the plot trajectory of each chapter
```{r}
library(tidyr)
jane_austen_sentiment <- tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(chpater, index = linenumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = chpater)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~chpater, ncol = 2, scales = "free_x")
```
```{r}
chapter.6 <- tidy_book %>% 
  filter(chpater == "6")

chapter.6
```
How the sentiment changes across the narrative arc of Chapter 6.
```{r}
afinn <- chapter.6 %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 10) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(chapter.6 %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing"),
                          chapter.6 %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
Both lexicons have more negative than positive words, but the ratio of negative to positive words is higher in the Bing lexicon than the NRC lexicon.   


Find out how much each word contributed to each sentiment.
```{r}
bing_word_count <- tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_count
```
```{r}
bing_word_count %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

Look at the most common words in book as a whole again
```{r}
library(wordcloud)

tidy_book %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```
Do the sentiment analysis to tag positive and negative words, then find the most common positive and negative words
```{r}
library(reshape2)

tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 5)
```
### book sites

http://freecomputerbooks.com/

https://en.wikisource.org/wiki/Main_Page

http://www.gutenberg.org/


## Text analysis links   


[CRAN Task View: Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)


[RcmdrPlugin.temis: Graphical Integrated Text Mining Solution](https://cran.r-project.org/web/packages/RcmdrPlugin.temis/index.html)

[Text mining with R github](https://github.com/dgrtwo/tidy-text-mining)


[r-break-corpus-into-sentences](https://stackoverflow.com/questions/18712878/r-break-corpus-into-sentences)

[tokenizers](https://cran.r-project.org/web/packages/tokenizers/README.html)


[Introduction to the tokenizers Package](https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html)



[Package ‘tokenizers’](https://cran.r-project.org/web/packages/tokenizers/tokenizers.pdf)











